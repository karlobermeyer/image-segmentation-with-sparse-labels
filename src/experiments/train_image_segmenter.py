#!/usr/bin/env python3
"""
Train a semantic image segmenter.

There are 2 options for training with this module:
(1) Import the function `train_image_segmenter` and call it from
    within Python code, or
(2) Execute this file as a script from the command line like this.
```
$ ./train_image_segmenter.py \
    --model MODEL \
    --hparams HPARAMS_FILENAME
```
For example, if you want to train DeepLabV3 using the hyperparameters in
`deeplabv3/hparams--smoke_test.yaml`, run this.
```
$ ./train_image_segmenter.py \
    --model deeplabv3 \
    --hparams deeplabv3/hparams--smoke_test.yaml
```

Results are stored as follows.
REPO_ROOT/runs/MODEL/SCENARIO/YYYY-MM-DDTHH_MM_SSZ/
├── epoch*-step*.ckpt
├── hparams.yaml
├── stderr.log
├── stdout.log
├── training_time_hrs.txt
│
├── training_time_series/  <-- Contents generated at end of run.
│   ├── metrics.txt   <-- Human readable.
│   ├── metrics.csv   <-- CSV version of `metrics.txt`.
│   ├── metrics_raw.csv   <-- Generated by Lightning, not very readable.
│   ├── loss_vs_epoch.png                   ┐
│   ├── loss_vs_epoch--with_title.png       │ Loss convergence plots
│   ├── log_loss_vs_epoch.png               │
│   └── log_loss_vs_epoch--with_title.png   ┘
│
└── final_model_evaluation/   <-- Empty except for a `README.md` after training.
    └── ...                       Generate subdirs to taste by running
                                  `evaluate_training_run.py`.
"""
# Standard
from argparse import ArgumentParser
from collections import Counter
import gc
import os
import pickle
import sys
import time
from typing import Dict, List, Optional, Union

# Numerics
import numpy as np

# Machine Learning
import torch
import albumentations as albm
#
import pytorch_lightning as pl
from pytorch_lightning.callbacks import (
    EarlyStopping,
    ModelCheckpoint,
)
from pytorch_lightning.loggers import CSVLogger

# Other
import watermark

# Project-Specific
from core import (
    SemanticClassId,
    SemanticClassMapping,
    current_utc_datetime_str,
    SparseLabelSimulatingDataset,
    compute_semantic_class_num_pixels,
    compute_semantic_class_frequencies,
    smooth_semantic_class_frequencies,
    compute_semantic_class_weights as compute_semantic_class_weights_,
)
from data.cityscapes import (
    class_mapping,
    preprocesses_from,
    subcityscapes_dataset,
    cityscapes_dataset,
    TargetClassNumPixelsCache,
)
from models import (
    ImageSegmenter,
    DeepLabV3ImageSegmenterHparams,
    deeplabv3_image_segmenter_hparams_from_yaml,
    DeepLabV3ImageSegmenter,
    SegformerImageSegmenterHparams,
    segformer_image_segmenter_hparams_from_yaml,
    SegformerImageSegmenter,
)
from experiments.training_accessories import (
    StdStreamLogger,
    TrainingRunPostprocessing,
)


REPO_ROOT: str = os.environ.get("REPO_ROOT")
assert REPO_ROOT is not None, \
    "REPO_ROOT not found! Did you run `setenv.sh`?"

CITYSCAPES_DATA_ROOT: str = os.path.join(REPO_ROOT, "data/cityscapes")

num_pixels_cache_filename: str = os.path.join(
    REPO_ROOT,
    "src/data/cityscapes/",
    "target_class_num_pixels_cache.pkl",
)
with open(num_pixels_cache_filename, "rb") as fin:
    TARGET_CLASS_NUM_PIXELS_CACHE: TargetClassNumPixelsCache = pickle.load(fin)


def compute_semantic_class_weights(
    class_mapping: SemanticClassMapping,
    dataset_train: SparseLabelSimulatingDataset,
    class_frequency_max_to_min_ratio_ubnd: Optional[float] = None,
) -> torch.Tensor:
    target_class_num_pixels: List[Counter[SemanticClassId]] = \
        dataset_train.target_class_num_pixels
    class_num_pixels: Counter[SemanticClassId] = \
        compute_semantic_class_num_pixels(
            class_mapping=class_mapping,
            target_class_num_pixels=target_class_num_pixels,
        )
    frequencies: np.ndarray = \
        compute_semantic_class_frequencies(class_num_pixels)
    if hparams.class_frequency_max_to_min_ratio_ubnd is not None:
        frequencies: np.ndarray = \
            smooth_semantic_class_frequencies(
                class_frequencies=frequencies,
                frequency_max_to_min_ratio_ubnd = \
                    hparams.class_frequency_max_to_min_ratio_ubnd,
            )
    return torch.Tensor(compute_semantic_class_weights_(frequencies))


def train_image_segmenter(
    hparams: Union[
        DeepLabV3ImageSegmenterHparams,
        SegformerImageSegmenterHparams,
    ],
    dataset_train: SparseLabelSimulatingDataset,
    dataset_val: Optional[SparseLabelSimulatingDataset] = None,
) -> Union[
    DeepLabV3ImageSegmenter,
    SegformerImageSegmenter,
]:
    """Train a semantic image segmenter."""
    start_time: float = time.time()

    assert hparams.model_name in ("deeplabv3", "segformer"), "Unknown model!"

    training_datetime_str: str = current_utc_datetime_str()
    run_dirname: str = os.path.join(
        REPO_ROOT,
        "runs/deeplabv3/",
        hparams.scenario_name,
        training_datetime_str,
    )
    os.makedirs(run_dirname, exist_ok=True)
    sys.stdout = StdStreamLogger(os.path.join(run_dirname, "stdout.log"))
    sys.stderr = StdStreamLogger(os.path.join(run_dirname, "stderr.log"))

    training_time_series_dirname: str = os.path.join(
        run_dirname,
        "training_time_series",
    )
    os.makedirs(training_time_series_dirname, exist_ok=True)

    platform_info = watermark.watermark()
    print(platform_info)

    # Compute semantic class weights.
    if hparams.weight_classes:
        semantic_class_weights: torch.Tensor = \
            compute_semantic_class_weights(
                class_mapping,
                dataset_train,
                hparams.class_frequency_max_to_min_ratio_ubnd,
            )
    else:
        semantic_class_weights = None

    # Construct image segmenter.
    if hparams.model_name == "deeplabv3":
        image_segmenter: ImageSegmenter = DeepLabV3ImageSegmenter(
            hparams=hparams,
            semantic_class_weights=semantic_class_weights,
            dataset_train=dataset_train,
            dataset_val=dataset_val,
        )
    elif hparams.model_name == "segformer":
        raise NotImplementedError
    else:
        raise NotImplementedError

    # The `name` argument of `CSVLogger` appears to be broken as of writing
    # this. Just let it write `hparams.yaml` and `metrics.csv` into
    # `run_dirname`, then `TrainingRunPostprocessing` will move it into
    # `{run_dirname}/training_time_series/metrics_raw.csv`.
    logger: CSVLogger = CSVLogger(
        save_dir=run_dirname,
        name=None,  # Leave as `None` as described above.
        version="",  # Union[int, str, None]
    )

    run_postprocessing: TrainingRunPostprocessing = \
        TrainingRunPostprocessing(run_dirname=run_dirname)

    if hparams.stop_early:
        assert dataset_val is not None
        checkpointer: ModelCheckpoint = ModelCheckpoint(
            monitor="val_loss",
            dirpath=run_dirname,
            filename="{epoch}-{step}",  # Extension `.ckpt` added automatically.
            save_last=None,
            save_top_k=1,
            mode="min",
        )
        early_stopper: EarlyStopping = EarlyStopping(
            monitor="val_loss",
            # Stop if `min_delta` not achieved for at least `patience`
            # iterations.
            min_delta=0.0,
            patience=3,
            verbose=True,
            mode="min",
            check_on_train_epoch_end=True,
        )
        callbacks: List[pl.Callback] = [
            checkpointer,
            early_stopper,
            run_postprocessing,
        ]
    else:
        checkpointer: ModelCheckpoint = ModelCheckpoint(
            monitor=None,
            dirpath=run_dirname,
            filename="{epoch}-{step}",  # Extension `.ckpt` added automatically.
            save_last=None,  # Leave as `None`, else saves redundant `last.ckpt`
            save_top_k=1,    # that is identical to the `epoch*-step*.ckpt`.
            mode="min",
        )
        callbacks: List[pl.Callback] = [checkpointer, run_postprocessing]

    if hparams.smoke_test:
        if dataset_val is None:
            limit_val_batches: int = 0
        else:
            limit_val_batches: int = 2
        trainer: pl.Trainer = pl.Trainer(
            logger=logger,
            default_root_dir=run_dirname,
            accelerator="gpu",  # Default: "auto"
            precision="16-mixed",
            enable_model_summary=True,
            enable_checkpointing=True,
            enable_progress_bar=True,
            log_every_n_steps=1,
            callbacks=callbacks,
            min_epochs=1,
            max_epochs=2,  # For short tests.
            limit_train_batches=2,  # For short tests.
            limit_val_batches=limit_val_batches,
            #fast_dev_run=True,  # Test single epoch only. Union[int, bool]
        )
    else:
        if dataset_val is None:
            limit_val_batches: int = 0
        else:
            limit_val_batches: float = 1.0
        trainer: pl.Trainer = pl.Trainer(
            logger=logger,
            default_root_dir=run_dirname,
            accelerator="gpu",  # Default: "auto"
            precision="16-mixed",
            enable_model_summary=True,
            enable_checkpointing=True,
            enable_progress_bar=True,
            log_every_n_steps=1,
            callbacks=callbacks,
            min_epochs=1,
            max_epochs=hparams.num_epochs_ubnd,
            limit_val_batches=limit_val_batches,
        )

    gc.collect()
    trainer.fit(image_segmenter)

    elapsed_time: float = time.time() - start_time
    training_time_hrs_str: str = f"{elapsed_time/3600.0:.3f}"
    training_time_filename: str = os.path.join(
        run_dirname,
        "training_time_hrs.txt",
    )
    with open(training_time_filename, "w") as file:
        file.write(training_time_hrs_str + "\n")
    print(f"\nElapsed time = {training_time_hrs_str} h")

    sys.stdout.log.close()
    sys.stderr.log.close()
    sys.stdout = sys.stdout.terminal
    sys.stderr = sys.stderr.terminal

    # Create a placeholder directory for evaluation results.
    final_model_evaluation_dirname: str = os.path.join(
        run_dirname,
        "final_model_evaluation",
    )
    os.makedirs(final_model_evaluation_dirname, exist_ok=True)
    with open(
        os.path.join(final_model_evaluation_dirname, "README.md"), "w",
    ) as fout:
        fout.write(
            "Use `evaluate_training_run.py` to generate and write "
            "evaluation results to subdirectories of this directory.\n"
        )

    return image_segmenter


if __name__ == "__main__":
    parser: ArgumentParser = ArgumentParser(
        description="Train a semantic image segmenter.",
    )
    parser.add_argument(
        "--model",
        type=str,
        help="Model name.",
        required=True,
    )
    parser.add_argument(
        "--hparams",
        type=str,
        help="Name of hyperparameters YAML file.",
        required=True,
    )
    args = parser.parse_args()

    assert args.model in ("deeplabv3", "segformer"), "Unknown model!"

    hparams_filename: str = args.hparams
    print(f"Loading hyperparameters from\n{hparams_filename}\n")
    hparams_filename: str = os.path.join(
        REPO_ROOT,
        "src/experiments/",
        hparams_filename,
    )
    if args.model == "deeplabv3":
        hparams: DeepLabV3ImageSegmenterHparams = \
            deeplabv3_image_segmenter_hparams_from_yaml(hparams_filename)
    elif args.model == "segformer":
        hparams: SegformerImageSegmenterHparams = \
            segformer_image_segmenter_hparams_from_yaml(hparams_filename)
    else:
        raise NotImplementedError
    assert hparams.model_name == args.model, \
        "Model does not match hyperparameters!"

    pl.seed_everything(hparams.seed)

    preprocesses: Dict[str, albm.Compose] = preprocesses_from(
        input_height=hparams.input_height,
        input_width=hparams.input_width,
        mean_for_input_normalization=hparams.mean_for_input_normalization,
        std_for_input_normalization=hparams.std_for_input_normalization,
        do_shift_scale_rotate=True,
        ignore_index=hparams.ignore_index,
    )

    # Construct training dataset.
    dataset_train_name: str = hparams.dataset_train_name
    if "subcityscapes" in dataset_train_name:
        dataset_train: SparseLabelSimulatingDataset = subcityscapes_dataset(
            split=dataset_train_name.split("_")[1],
            preprocess=preprocesses["train"],
            len_scale_factor=hparams.len_dataset_train_scale_factor,
            label_density=hparams.label_density,
            ignore_index=hparams.ignore_index,
            shuffle=False,
            target_class_num_pixels=TARGET_CLASS_NUM_PIXELS_CACHE.get(
                dataset_name=dataset_train_name,
                len_scale_factor=hparams.len_dataset_train_scale_factor,
                label_density=hparams.label_density,
            ),
        )
    else:
        dataset_train: SparseLabelSimulatingDataset = cityscapes_dataset(
            split=dataset_train_name.split("_")[1],
            preprocess=preprocesses["train"],
            len_scale_factor=hparams.len_dataset_train_scale_factor,
            label_density=hparams.label_density,
            ignore_index=hparams.ignore_index,
            shuffle=False,
            target_class_num_pixels=TARGET_CLASS_NUM_PIXELS_CACHE.get(
                dataset_name=dataset_train_name,
                len_scale_factor=hparams.len_dataset_train_scale_factor,
                label_density=hparams.label_density,
            ),
        )

    # Construct validation dataset.
    dataset_val_name: Optional[str] = hparams.dataset_val_name
    if dataset_val_name is None:
        dataset_val = None
    elif "subcityscapes" in dataset_val_name:
        dataset_val: SparseLabelSimulatingDataset = subcityscapes_dataset(
            split=dataset_val_name.split("_")[1],
            preprocess=preprocesses["infer"],
            len_scale_factor=hparams.len_dataset_val_scale_factor,
            label_density=hparams.label_density,
            ignore_index=hparams.ignore_index,
            shuffle=False,
            target_class_num_pixels=TARGET_CLASS_NUM_PIXELS_CACHE.get(
                dataset_name=dataset_val_name,
                len_scale_factor=hparams.len_dataset_val_scale_factor,
                label_density=hparams.label_density,
            ),
        )
    else:
        assert dataset_val_name.startswith("cityscapes")
        dataset_val: SparseLabelSimulatingDataset = cityscapes_dataset(
            split=dataset_val_name.split("_")[1],
            preprocess=preprocesses["infer"],
            len_scale_factor=hparams.len_dataset_val_scale_factor,
            label_density=hparams.label_density,
            ignore_index=hparams.ignore_index,
            shuffle=False,
            target_class_num_pixels=TARGET_CLASS_NUM_PIXELS_CACHE.get(
                dataset_name=dataset_val_name,
                len_scale_factor=hparams.len_dataset_val_scale_factor,
                label_density=hparams.label_density,
            ),
        )

    _: Union[
        DeepLabV3ImageSegmenter,
        SegformerImageSegmenter,
    ] = train_image_segmenter(
        hparams,
        dataset_train,
        dataset_val,
    )
